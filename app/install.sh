#!/bin/bash
# =============================================================================
# Titier - Script de InstalaÃ§Ã£o de DependÃªncias
# Configura llama-cpp-python com suporte a Metal (macOS) ou CUDA (Windows/Linux)
# =============================================================================

set -e

echo "========================================"
echo "Titier - InstalaÃ§Ã£o de DependÃªncias"
echo "========================================"

# Detectar sistema operacional
detect_os() {
    case "$(uname -s)" in
        Darwin*)    echo "macos" ;;
        Linux*)     echo "linux" ;;
        CYGWIN*|MINGW*|MSYS*) echo "windows" ;;
        *)          echo "unknown" ;;
    esac
}

OS=$(detect_os)
echo "Sistema detectado: $OS"

# Verificar Python
if ! command -v python3 &> /dev/null; then
    echo "âŒ Python 3 nÃ£o encontrado. Por favor, instale Python 3.11+"
    exit 1
fi

PYTHON_VERSION=$(python3 -c 'import sys; print(f"{sys.version_info.major}.{sys.version_info.minor}")')
echo "Python: $PYTHON_VERSION"

# Verificar Poetry
if ! command -v poetry &> /dev/null; then
    echo "âš ï¸  Poetry nÃ£o encontrado. Instalando via pipx..."
    pip3 install pipx
    pipx install poetry
fi

# Ir para diretÃ³rio do app
cd "$(dirname "$0")"

echo ""
echo "ðŸ“¦ Instalando dependÃªncias base..."
poetry install

echo ""
echo "ðŸ”§ Configurando llama-cpp-python com aceleraÃ§Ã£o GPU..."

case "$OS" in
    macos)
        echo "   â†’ Compilando com suporte a Metal (Apple Silicon)..."
        CMAKE_ARGS="-DGGML_METAL=on" poetry run pip install llama-cpp-python --force-reinstall --no-cache-dir
        ;;
    windows)
        echo "   â†’ Compilando com suporte a CUDA (NVIDIA)..."
        echo "   âš ï¸  Certifique-se de que o CUDA Toolkit estÃ¡ instalado!"
        CMAKE_ARGS="-DGGML_CUDA=on" poetry run pip install llama-cpp-python --force-reinstall --no-cache-dir
        ;;
    linux)
        # Detectar se tem NVIDIA GPU
        if command -v nvidia-smi &> /dev/null; then
            echo "   â†’ GPU NVIDIA detectada, compilando com CUDA..."
            CMAKE_ARGS="-DGGML_CUDA=on" poetry run pip install llama-cpp-python --force-reinstall --no-cache-dir
        else
            echo "   â†’ Sem GPU NVIDIA, usando CPU..."
            poetry run pip install llama-cpp-python --force-reinstall --no-cache-dir
        fi
        ;;
    *)
        echo "   â†’ Sistema desconhecido, instalando versÃ£o CPU..."
        poetry run pip install llama-cpp-python --force-reinstall --no-cache-dir
        ;;
esac

echo ""
echo "âœ… Verificando instalaÃ§Ã£o..."
poetry run python -c "from app.core.inference import check_installation; check_installation()"

echo ""
echo "========================================"
echo "âœ… InstalaÃ§Ã£o concluÃ­da!"
echo "========================================"
echo ""
echo "PrÃ³ximos passos:"
echo "  1. Baixe um modelo GGUF (ex: Llama-3.1-8B-Q4_K_M.gguf)"
echo "  2. Coloque em ~/.titier/models/"
echo "  3. Execute: poetry run python server.py"
